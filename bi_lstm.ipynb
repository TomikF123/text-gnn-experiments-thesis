{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim  \n",
    "import torch.nn.functional as F  \n",
    "from torch.utils.data import DataLoader, Dataset  \n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_all = pd.read_csv('data/mr_all.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"./data\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])  # Remove punctuation\n",
    "    tokens = text.split()  # Tokenize\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens  # return as list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_all[\"tokens\"] = mr_all[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 113569, Unique words: 20359\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in mr_all[\"tokens\"] for word in tokens]\n",
    "print(f\"Total tokens: {len(all_words)}, Unique words: {len(set(all_words))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock',\n",
       " 'destined',\n",
       " '21st',\n",
       " 'centurys',\n",
       " 'new',\n",
       " 'conan',\n",
       " 'hes',\n",
       " 'going',\n",
       " 'make',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'arnold',\n",
       " 'schwarzenegger',\n",
       " 'jeanclaud',\n",
       " 'van',\n",
       " 'damme',\n",
       " 'steven',\n",
       " 'segal',\n",
       " 'gorgeously',\n",
       " 'elaborate',\n",
       " 'continuation',\n",
       " 'lord',\n",
       " 'rings',\n",
       " 'trilogy',\n",
       " 'huge',\n",
       " 'column',\n",
       " 'words',\n",
       " 'cannot',\n",
       " 'adequately',\n",
       " 'describe',\n",
       " 'cowriterdirector',\n",
       " 'peter',\n",
       " 'jacksons',\n",
       " 'expanded',\n",
       " 'vision',\n",
       " 'j',\n",
       " 'r',\n",
       " 'r',\n",
       " 'tolkiens',\n",
       " 'middleearth',\n",
       " 'effective',\n",
       " 'tootepid',\n",
       " 'biopic',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'go',\n",
       " 'movies',\n",
       " 'fun',\n",
       " 'wasabi',\n",
       " 'good',\n",
       " 'place',\n",
       " 'start',\n",
       " 'emerges',\n",
       " 'something',\n",
       " 'rare',\n",
       " 'issue',\n",
       " 'movie',\n",
       " 'thats',\n",
       " 'honest',\n",
       " 'keenly',\n",
       " 'observed',\n",
       " 'doesnt',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'one',\n",
       " 'film',\n",
       " 'provides',\n",
       " 'great',\n",
       " 'insight',\n",
       " 'neurotic',\n",
       " 'mindset',\n",
       " 'comics',\n",
       " 'even',\n",
       " 'reached',\n",
       " 'absolute',\n",
       " 'top',\n",
       " 'game',\n",
       " 'offers',\n",
       " 'rare',\n",
       " 'combination',\n",
       " 'entertainment',\n",
       " 'education',\n",
       " 'perhaps',\n",
       " 'picture',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'literally',\n",
       " 'showed',\n",
       " 'road',\n",
       " 'hell',\n",
       " 'paved',\n",
       " 'good',\n",
       " 'intentions',\n",
       " 'steers',\n",
       " 'turns',\n",
       " 'snappy',\n",
       " 'screenplay',\n",
       " 'curls',\n",
       " 'edges',\n",
       " 'clever',\n",
       " 'want',\n",
       " 'hate',\n",
       " 'somehow',\n",
       " 'pulls',\n",
       " 'take',\n",
       " 'care',\n",
       " 'cat',\n",
       " 'offers',\n",
       " 'refreshingly',\n",
       " 'different',\n",
       " 'slice',\n",
       " 'asian',\n",
       " 'cinema',\n",
       " 'film',\n",
       " 'well',\n",
       " 'worth',\n",
       " 'seeing',\n",
       " 'talking',\n",
       " 'singing',\n",
       " 'heads',\n",
       " 'really',\n",
       " 'surprises',\n",
       " 'wisegirls',\n",
       " 'lowkey',\n",
       " 'quality',\n",
       " 'genuine',\n",
       " 'tenderness',\n",
       " 'wendigo',\n",
       " 'go',\n",
       " 'cinema',\n",
       " 'fed',\n",
       " 'eye',\n",
       " 'heart',\n",
       " 'mind',\n",
       " 'one',\n",
       " 'greatest',\n",
       " 'familyoriented',\n",
       " 'fantasyadventure',\n",
       " 'movies',\n",
       " 'ever',\n",
       " 'ultimately',\n",
       " 'ponders',\n",
       " 'reasons',\n",
       " 'need',\n",
       " 'stories',\n",
       " 'much',\n",
       " 'utterly',\n",
       " 'compelling',\n",
       " 'wrote',\n",
       " 'reputation',\n",
       " 'famous',\n",
       " 'author',\n",
       " 'ever',\n",
       " 'lived',\n",
       " 'comes',\n",
       " 'question',\n",
       " 'illuminating',\n",
       " 'overly',\n",
       " 'talky',\n",
       " 'documentary',\n",
       " 'masterpiece',\n",
       " 'four',\n",
       " 'years',\n",
       " 'making',\n",
       " 'movies',\n",
       " 'ripe',\n",
       " 'enrapturing',\n",
       " 'beauty',\n",
       " 'tempt',\n",
       " 'willing',\n",
       " 'probe',\n",
       " 'inscrutable',\n",
       " 'mysteries',\n",
       " 'offers',\n",
       " 'breath',\n",
       " 'fresh',\n",
       " 'air',\n",
       " 'true',\n",
       " 'sophistication',\n",
       " 'thoughtful',\n",
       " 'provocative',\n",
       " 'insistently',\n",
       " 'humanizing',\n",
       " 'film',\n",
       " 'cast',\n",
       " 'includes',\n",
       " 'top',\n",
       " 'actors',\n",
       " 'working',\n",
       " 'independent',\n",
       " 'film',\n",
       " 'lovely',\n",
       " 'amazing',\n",
       " 'involves',\n",
       " 'us',\n",
       " 'incisive',\n",
       " 'bleakly',\n",
       " 'amusing',\n",
       " 'go',\n",
       " 'lives',\n",
       " 'disturbing',\n",
       " 'frighteningly',\n",
       " 'evocative',\n",
       " 'assembly',\n",
       " 'imagery',\n",
       " 'hypnotic',\n",
       " 'music',\n",
       " 'composed',\n",
       " 'philip',\n",
       " 'glass',\n",
       " 'everyone',\n",
       " 'connect',\n",
       " 'nice',\n",
       " 'departure',\n",
       " 'standard',\n",
       " 'moviegoing',\n",
       " 'fare',\n",
       " 'scores',\n",
       " 'points',\n",
       " 'dedicated',\n",
       " 'goodhearted',\n",
       " 'professionalism',\n",
       " 'occasionally',\n",
       " 'melodramatic',\n",
       " 'also',\n",
       " 'extremely',\n",
       " 'effective',\n",
       " 'spiderman',\n",
       " 'rocks',\n",
       " 'idealistic',\n",
       " 'love',\n",
       " 'story',\n",
       " 'brings',\n",
       " 'latent',\n",
       " '15yearold',\n",
       " 'romantic',\n",
       " 'everyone',\n",
       " '95',\n",
       " 'minutes',\n",
       " 'treasure',\n",
       " 'planet',\n",
       " 'maintains',\n",
       " 'brisk',\n",
       " 'pace',\n",
       " 'races',\n",
       " 'familiar',\n",
       " 'story',\n",
       " 'however',\n",
       " 'lacks',\n",
       " 'grandeur',\n",
       " 'epic',\n",
       " 'quality',\n",
       " 'often',\n",
       " 'associated',\n",
       " 'stevensons',\n",
       " 'tale',\n",
       " 'well',\n",
       " 'earlier',\n",
       " 'disney',\n",
       " 'efforts',\n",
       " 'helps',\n",
       " 'lil',\n",
       " 'bow',\n",
       " 'wow',\n",
       " 'tones',\n",
       " 'pintsized',\n",
       " 'gangsta',\n",
       " 'act',\n",
       " 'play',\n",
       " 'someone',\n",
       " 'resembles',\n",
       " 'real',\n",
       " 'kid',\n",
       " 'guaranteed',\n",
       " 'move',\n",
       " 'anyone',\n",
       " 'ever',\n",
       " 'shook',\n",
       " 'rattled',\n",
       " 'rolled',\n",
       " 'masterful',\n",
       " 'film',\n",
       " 'master',\n",
       " 'filmmaker',\n",
       " 'unique',\n",
       " 'deceptive',\n",
       " 'grimness',\n",
       " 'compelling',\n",
       " 'fatalist',\n",
       " 'worldview',\n",
       " 'light',\n",
       " 'cute',\n",
       " 'forgettable',\n",
       " 'theres',\n",
       " 'way',\n",
       " 'effectively',\n",
       " 'teach',\n",
       " 'kids',\n",
       " 'dangers',\n",
       " 'drugs',\n",
       " 'think',\n",
       " 'projects',\n",
       " 'like',\n",
       " 'unfortunately',\n",
       " 'rrated',\n",
       " 'paid',\n",
       " 'would',\n",
       " 'easy',\n",
       " 'give',\n",
       " 'crush',\n",
       " 'new',\n",
       " 'title',\n",
       " 'two',\n",
       " 'weddings',\n",
       " 'funeral',\n",
       " 'far',\n",
       " 'thoughtful',\n",
       " 'film',\n",
       " 'slice',\n",
       " 'hugh',\n",
       " 'grant',\n",
       " 'whimsy',\n",
       " 'though',\n",
       " 'everything',\n",
       " 'might',\n",
       " 'literate',\n",
       " 'smart',\n",
       " 'never',\n",
       " 'took',\n",
       " 'always',\n",
       " 'seemed',\n",
       " 'static',\n",
       " 'cantet',\n",
       " 'perfectly',\n",
       " 'captures',\n",
       " 'hotel',\n",
       " 'lobbies',\n",
       " 'twolane',\n",
       " 'highways',\n",
       " 'roadside',\n",
       " 'cafes',\n",
       " 'permeate',\n",
       " 'vincents',\n",
       " 'days',\n",
       " 'ms',\n",
       " 'fulfordwierzbicki',\n",
       " 'almost',\n",
       " 'spooky',\n",
       " 'sulky',\n",
       " 'calculating',\n",
       " 'lolita',\n",
       " 'turn',\n",
       " 'though',\n",
       " 'means',\n",
       " 'best',\n",
       " 'work',\n",
       " 'laissezpasser',\n",
       " 'distinguished',\n",
       " 'distinctive',\n",
       " 'effort',\n",
       " 'bonafide',\n",
       " 'master',\n",
       " 'fascinating',\n",
       " 'film',\n",
       " 'replete',\n",
       " 'rewards',\n",
       " 'willing',\n",
       " 'make',\n",
       " 'effort',\n",
       " 'reap',\n",
       " 'like',\n",
       " 'bond',\n",
       " 'outings',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'stunts',\n",
       " 'outlandish',\n",
       " 'border',\n",
       " 'cartoonlike',\n",
       " 'heavy',\n",
       " 'reliance',\n",
       " 'cgi',\n",
       " 'technology',\n",
       " 'beginning',\n",
       " 'creep',\n",
       " 'series',\n",
       " 'newton',\n",
       " 'draws',\n",
       " 'attention',\n",
       " 'like',\n",
       " 'magnet',\n",
       " 'acts',\n",
       " 'circles',\n",
       " 'around',\n",
       " 'better',\n",
       " 'known',\n",
       " 'costar',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " 'story',\n",
       " 'loses',\n",
       " 'bite',\n",
       " 'lastminute',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'thats',\n",
       " 'even',\n",
       " 'less',\n",
       " 'plausible',\n",
       " 'rest',\n",
       " 'picture',\n",
       " 'much',\n",
       " 'way',\n",
       " 'though',\n",
       " 'refreshingly',\n",
       " 'novel',\n",
       " 'ride',\n",
       " 'fuller',\n",
       " 'would',\n",
       " 'surely',\n",
       " 'called',\n",
       " 'gutsy',\n",
       " 'times',\n",
       " 'exhilarating',\n",
       " 'movie',\n",
       " 'great',\n",
       " 'yarn',\n",
       " 'compleja',\n",
       " 'e',\n",
       " 'intelectualmente',\n",
       " 'retadora',\n",
       " 'el',\n",
       " 'ladrón',\n",
       " 'de',\n",
       " 'orquídeas',\n",
       " 'es',\n",
       " 'uno',\n",
       " 'de',\n",
       " 'esos',\n",
       " 'filmes',\n",
       " 'que',\n",
       " 'vale',\n",
       " 'la',\n",
       " 'pena',\n",
       " 'ver',\n",
       " 'precisamente',\n",
       " 'por',\n",
       " 'su',\n",
       " 'originalidad',\n",
       " 'film',\n",
       " 'makes',\n",
       " 'strong',\n",
       " 'case',\n",
       " 'importance',\n",
       " 'musicians',\n",
       " 'creating',\n",
       " 'motown',\n",
       " 'sound',\n",
       " 'karmen',\n",
       " 'moves',\n",
       " 'like',\n",
       " 'rhythm',\n",
       " 'lips',\n",
       " 'chanting',\n",
       " 'beat',\n",
       " 'long',\n",
       " 'braided',\n",
       " 'hair',\n",
       " 'little',\n",
       " 'wipe',\n",
       " 'away',\n",
       " 'jeweled',\n",
       " 'beads',\n",
       " 'sweat',\n",
       " 'gosling',\n",
       " 'provides',\n",
       " 'amazing',\n",
       " 'performance',\n",
       " 'dwarfs',\n",
       " 'everything',\n",
       " 'else',\n",
       " 'film',\n",
       " 'real',\n",
       " 'movie',\n",
       " 'real',\n",
       " 'people',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'rare',\n",
       " 'glimpse',\n",
       " 'culture',\n",
       " 'us',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'tender',\n",
       " 'yet',\n",
       " 'lacerating',\n",
       " 'darkly',\n",
       " 'funny',\n",
       " 'fable',\n",
       " 'may',\n",
       " 'spoofing',\n",
       " 'easy',\n",
       " 'target',\n",
       " 'old',\n",
       " '50s',\n",
       " 'giant',\n",
       " 'creature',\n",
       " 'features',\n",
       " 'acknowledges',\n",
       " 'celebrates',\n",
       " 'cheesiness',\n",
       " 'reason',\n",
       " 'people',\n",
       " 'get',\n",
       " 'kick',\n",
       " 'watching',\n",
       " 'today',\n",
       " 'engaging',\n",
       " 'overview',\n",
       " 'johnsons',\n",
       " 'eccentric',\n",
       " 'career',\n",
       " 'ragged',\n",
       " 'cheap',\n",
       " 'unassuming',\n",
       " 'way',\n",
       " 'movie',\n",
       " 'works',\n",
       " 'actors',\n",
       " 'much',\n",
       " 'charisma',\n",
       " 'youd',\n",
       " 'happy',\n",
       " 'listen',\n",
       " 'reading',\n",
       " 'phone',\n",
       " 'book',\n",
       " 'hugh',\n",
       " 'grant',\n",
       " 'sandra',\n",
       " 'bullock',\n",
       " 'two',\n",
       " 'likeable',\n",
       " 'actors',\n",
       " 'sandra',\n",
       " 'nettelbeck',\n",
       " 'beautifully',\n",
       " 'orchestrates',\n",
       " 'transformation',\n",
       " 'chilly',\n",
       " 'neurotic',\n",
       " 'selfabsorbed',\n",
       " 'martha',\n",
       " 'heart',\n",
       " 'begins',\n",
       " 'open',\n",
       " 'behind',\n",
       " 'snow',\n",
       " 'games',\n",
       " 'lovable',\n",
       " 'siberian',\n",
       " 'huskies',\n",
       " 'plus',\n",
       " 'one',\n",
       " 'sheep',\n",
       " 'dog',\n",
       " 'picture',\n",
       " 'hosts',\n",
       " 'parkawrapped',\n",
       " 'dose',\n",
       " 'heart',\n",
       " 'everytime',\n",
       " 'think',\n",
       " 'undercover',\n",
       " 'brother',\n",
       " 'run',\n",
       " 'steam',\n",
       " 'finds',\n",
       " 'new',\n",
       " 'way',\n",
       " 'surprise',\n",
       " 'amuse',\n",
       " 'manages',\n",
       " 'original',\n",
       " 'even',\n",
       " 'though',\n",
       " 'rips',\n",
       " 'many',\n",
       " 'ideas',\n",
       " 'singercomposer',\n",
       " 'bryan',\n",
       " 'adams',\n",
       " 'contributes',\n",
       " 'slew',\n",
       " 'songs',\n",
       " '\\x97',\n",
       " 'potential',\n",
       " 'hits',\n",
       " 'simply',\n",
       " 'intrusive',\n",
       " 'story',\n",
       " '\\x97',\n",
       " 'whole',\n",
       " 'package',\n",
       " 'certainly',\n",
       " 'captures',\n",
       " 'intended',\n",
       " 'er',\n",
       " 'spirit',\n",
       " 'piece',\n",
       " 'youd',\n",
       " 'think',\n",
       " 'america',\n",
       " 'would',\n",
       " 'enough',\n",
       " 'plucky',\n",
       " 'british',\n",
       " 'eccentrics',\n",
       " 'hearts',\n",
       " 'gold',\n",
       " 'yet',\n",
       " 'act',\n",
       " 'still',\n",
       " 'charming',\n",
       " 'whether',\n",
       " 'youre',\n",
       " 'enlightened',\n",
       " 'derridas',\n",
       " 'lectures',\n",
       " 'self',\n",
       " 'derrida',\n",
       " 'undeniably',\n",
       " 'fascinating',\n",
       " 'playful',\n",
       " 'fellow',\n",
       " 'pleasant',\n",
       " 'enough',\n",
       " 'movie',\n",
       " 'held',\n",
       " 'together',\n",
       " 'skilled',\n",
       " 'ensemble',\n",
       " 'actors',\n",
       " 'best',\n",
       " 'american',\n",
       " 'movie',\n",
       " 'troubled',\n",
       " 'teens',\n",
       " 'since',\n",
       " '1998s',\n",
       " 'whatever',\n",
       " 'disney',\n",
       " 'always',\n",
       " 'hitormiss',\n",
       " 'bringing',\n",
       " 'beloved',\n",
       " 'kids',\n",
       " 'books',\n",
       " 'screen',\n",
       " 'tuck',\n",
       " 'everlasting',\n",
       " 'little',\n",
       " 'labour',\n",
       " 'involved',\n",
       " 'creating',\n",
       " 'layered',\n",
       " 'richness',\n",
       " 'imagery',\n",
       " 'chiaroscuro',\n",
       " 'madness',\n",
       " 'light',\n",
       " 'astonishing',\n",
       " 'animated',\n",
       " 'subplot',\n",
       " 'keenly',\n",
       " 'depicts',\n",
       " 'inner',\n",
       " 'struggles',\n",
       " 'adolescent',\n",
       " 'heroes',\n",
       " 'insecure',\n",
       " 'uncontrolled',\n",
       " 'intense',\n",
       " 'invincible',\n",
       " 'werner',\n",
       " 'herzog',\n",
       " 'alive',\n",
       " 'well',\n",
       " 'living',\n",
       " 'la',\n",
       " 'morton',\n",
       " 'great',\n",
       " 'actress',\n",
       " 'portraying',\n",
       " 'complex',\n",
       " 'character',\n",
       " 'morvern',\n",
       " 'callar',\n",
       " 'grows',\n",
       " 'less',\n",
       " 'compelling',\n",
       " 'farther',\n",
       " 'meanders',\n",
       " 'shocking',\n",
       " 'start',\n",
       " 'part',\n",
       " 'charm',\n",
       " 'satin',\n",
       " 'rouge',\n",
       " 'avoids',\n",
       " 'obvious',\n",
       " 'humour',\n",
       " 'lightness',\n",
       " 'son',\n",
       " 'bride',\n",
       " 'may',\n",
       " 'good',\n",
       " 'halfhour',\n",
       " 'long',\n",
       " 'comes',\n",
       " 'replete',\n",
       " 'flattering',\n",
       " 'sense',\n",
       " 'mystery',\n",
       " 'quietness',\n",
       " 'simmering',\n",
       " 'psychological',\n",
       " 'drama',\n",
       " 'bursts',\n",
       " 'sudden',\n",
       " 'violence',\n",
       " 'startling',\n",
       " 'slow',\n",
       " 'buildup',\n",
       " 'preceded',\n",
       " 'taut',\n",
       " 'intelligent',\n",
       " 'psychological',\n",
       " 'drama',\n",
       " 'compelling',\n",
       " 'comingofage',\n",
       " 'drama',\n",
       " 'arduous',\n",
       " 'journey',\n",
       " 'sensitive',\n",
       " 'young',\n",
       " 'girl',\n",
       " 'series',\n",
       " 'foster',\n",
       " 'homes',\n",
       " 'fierce',\n",
       " 'struggle',\n",
       " 'pull',\n",
       " 'free',\n",
       " 'dangerous',\n",
       " 'domineering',\n",
       " 'mothers',\n",
       " 'hold',\n",
       " 'truly',\n",
       " 'moving',\n",
       " 'experience',\n",
       " 'perfect',\n",
       " 'example',\n",
       " 'art',\n",
       " 'done',\n",
       " 'right',\n",
       " 'help',\n",
       " 'heal',\n",
       " 'clarify',\n",
       " 'comfort',\n",
       " 'delicately',\n",
       " 'observed',\n",
       " 'story',\n",
       " 'deeply',\n",
       " 'felt',\n",
       " 'masterfully',\n",
       " 'stylized',\n",
       " 'triumph',\n",
       " 'maverick',\n",
       " 'director',\n",
       " 'heart',\n",
       " 'movie',\n",
       " 'deftly',\n",
       " 'wrought',\n",
       " 'suspense',\n",
       " 'yarn',\n",
       " 'whose',\n",
       " 'richer',\n",
       " 'shadings',\n",
       " 'work',\n",
       " 'coloring',\n",
       " 'rather',\n",
       " 'substance',\n",
       " 'appearance',\n",
       " 'treebeard',\n",
       " 'gollums',\n",
       " 'expanded',\n",
       " 'role',\n",
       " 'either',\n",
       " 'loving',\n",
       " 'youre',\n",
       " 'seeing',\n",
       " 'rolling',\n",
       " 'eyes',\n",
       " 'loved',\n",
       " 'gollums',\n",
       " 'performance',\n",
       " 'incredible',\n",
       " 'screenplay',\n",
       " 'ingeniously',\n",
       " 'constructed',\n",
       " 'memento',\n",
       " 'movie',\n",
       " 'book',\n",
       " 'would',\n",
       " 'pageturner',\n",
       " 'cant',\n",
       " 'wait',\n",
       " 'see',\n",
       " 'happens',\n",
       " 'next',\n",
       " 'haneke',\n",
       " 'challenges',\n",
       " 'us',\n",
       " 'confront',\n",
       " 'reality',\n",
       " 'sexual',\n",
       " 'aberration',\n",
       " 'absorbing',\n",
       " 'disturbing',\n",
       " 'perhaps',\n",
       " 'disturbing',\n",
       " 'originally',\n",
       " 'intended',\n",
       " 'little',\n",
       " 'clarity',\n",
       " 'would',\n",
       " 'gone',\n",
       " 'long',\n",
       " 'way',\n",
       " 'best',\n",
       " 'film',\n",
       " 'year',\n",
       " 'far',\n",
       " 'benchmark',\n",
       " 'best',\n",
       " 'picture',\n",
       " 'contenders',\n",
       " 'measured',\n",
       " 'painful',\n",
       " 'watch',\n",
       " 'viewers',\n",
       " 'willing',\n",
       " 'take',\n",
       " 'chance',\n",
       " 'rewarded',\n",
       " 'two',\n",
       " 'years',\n",
       " 'accomplished',\n",
       " 'riveting',\n",
       " 'film',\n",
       " 'performances',\n",
       " 'startling',\n",
       " 'film',\n",
       " 'gives',\n",
       " 'fascinating',\n",
       " 'albeit',\n",
       " 'depressing',\n",
       " 'view',\n",
       " 'iranian',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'close',\n",
       " 'iraqi',\n",
       " 'border',\n",
       " 'imaginative',\n",
       " 'comedythriller',\n",
       " 'artsy',\n",
       " 'flourishes',\n",
       " 'aside',\n",
       " 'narc',\n",
       " 'gritty',\n",
       " 'movie',\n",
       " 'gets',\n",
       " 'days',\n",
       " 'isle',\n",
       " 'preposterous',\n",
       " 'thoroughly',\n",
       " 'misogynistic',\n",
       " 'vistas',\n",
       " 'incredibly',\n",
       " 'beautiful',\n",
       " 'look',\n",
       " 'together',\n",
       " 'tok',\n",
       " 'orchestrate',\n",
       " 'buoyant',\n",
       " 'darkly',\n",
       " 'funny',\n",
       " 'dance',\n",
       " 'death',\n",
       " 'process',\n",
       " 'demonstrate',\n",
       " 'theres',\n",
       " 'still',\n",
       " 'lot',\n",
       " 'life',\n",
       " 'hong',\n",
       " 'kong',\n",
       " 'cinema',\n",
       " 'director',\n",
       " 'kapur',\n",
       " 'filmmaker',\n",
       " 'real',\n",
       " 'flair',\n",
       " 'epic',\n",
       " 'landscapes',\n",
       " 'adventure',\n",
       " 'better',\n",
       " 'film',\n",
       " 'earlier',\n",
       " 'englishlanguage',\n",
       " 'movie',\n",
       " 'overpraised',\n",
       " 'elizabeth',\n",
       " 'movie',\n",
       " 'blast',\n",
       " 'educational',\n",
       " 'energy',\n",
       " 'bouncy',\n",
       " 'animation',\n",
       " 'catchy',\n",
       " 'songs',\n",
       " 'escort',\n",
       " 'entire',\n",
       " '85',\n",
       " 'minutes',\n",
       " 'sports',\n",
       " 'movie',\n",
       " 'action',\n",
       " 'thats',\n",
       " 'exciting',\n",
       " 'field',\n",
       " 'story',\n",
       " 'care',\n",
       " 'doug',\n",
       " 'liman',\n",
       " 'director',\n",
       " 'bourne',\n",
       " 'directs',\n",
       " 'traffic',\n",
       " 'well',\n",
       " 'gets',\n",
       " 'nice',\n",
       " 'wintry',\n",
       " 'look',\n",
       " 'locations',\n",
       " 'absorbs',\n",
       " 'us',\n",
       " 'movies',\n",
       " 'spycraft',\n",
       " 'uses',\n",
       " 'damons',\n",
       " 'ability',\n",
       " 'focused',\n",
       " 'sincere',\n",
       " 'tenderness',\n",
       " 'piece',\n",
       " 'still',\n",
       " 'intact',\n",
       " 'katz',\n",
       " 'uses',\n",
       " 'archival',\n",
       " 'footage',\n",
       " 'horrifying',\n",
       " 'documents',\n",
       " 'lynchings',\n",
       " 'still',\n",
       " 'photographs',\n",
       " 'charming',\n",
       " 'old',\n",
       " 'reeltoreel',\n",
       " 'recordings',\n",
       " 'meeropol',\n",
       " 'entertaining',\n",
       " 'children',\n",
       " 'create',\n",
       " 'song',\n",
       " 'history',\n",
       " 'powerful',\n",
       " 'song',\n",
       " 'like',\n",
       " 'films',\n",
       " 'almost',\n",
       " 'anthropologically',\n",
       " 'detailed',\n",
       " 'realization',\n",
       " 'early80s',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_all\n",
    "all_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "mr_all[\"label\"] = le.fit_transform(mr_all[\"label\"])  # Now 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rock, destined, 21st, centurys, new, conan, h...</td>\n",
       "      <td>[533, 2417, 3118, 10163, 34, 7008, 215, 148, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[gorgeously, elaborate, continuation, lord, ri...</td>\n",
       "      <td>[3119, 1951, 7010, 4374, 2724, 4375, 948, 7011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "      <td>[effective, tootepid, biopic]</td>\n",
       "      <td>[562, 10167, 1785]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[sometimes, like, go, movies, fun, wasabi, goo...</td>\n",
       "      <td>[152, 5, 107, 18, 56, 7014, 9, 212, 535]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[emerges, something, rare, issue, movie, thats...</td>\n",
       "      <td>[1205, 36, 276, 1641, 3, 41, 399, 5349, 3663, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>this picture is murder by numbers , and as eas...</td>\n",
       "      <td>0</td>\n",
       "      <td>[picture, murder, numbers, easy, bored, abcs, ...</td>\n",
       "      <td>[84, 707, 1201, 277, 1546, 20356, 113, 20357, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>hilarious musical comedy though stymied by acc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hilarious, musical, comedy, though, stymied, ...</td>\n",
       "      <td>[370, 1231, 10, 59, 20358, 2789, 2134, 9050]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>if you are into splatter movies , then you wil...</td>\n",
       "      <td>0</td>\n",
       "      <td>[splatter, movies, probably, reasonably, good,...</td>\n",
       "      <td>[20359, 18, 227, 1456, 9, 11, 4260, 1042]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>a dull , simple-minded and stereotypical tale ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dull, simpleminded, stereotypical, tale, drug...</td>\n",
       "      <td>[172, 4341, 3626, 87, 1792, 343, 20360, 6979, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>the feature-length stretch . . . strains the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[featurelength, stretch, strains, shows, concept]</td>\n",
       "      <td>[2154, 6673, 3450, 313, 504]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      the rock is destined to be the 21st century's ...      1   \n",
       "1      the gorgeously elaborate continuation of \" the...      1   \n",
       "2                         effective but too-tepid biopic      1   \n",
       "3      if you sometimes like to go to the movies to h...      1   \n",
       "4      emerges as something rare , an issue movie tha...      1   \n",
       "...                                                  ...    ...   \n",
       "10657  this picture is murder by numbers , and as eas...      0   \n",
       "10658  hilarious musical comedy though stymied by acc...      0   \n",
       "10659  if you are into splatter movies , then you wil...      0   \n",
       "10660  a dull , simple-minded and stereotypical tale ...      0   \n",
       "10661  the feature-length stretch . . . strains the s...      0   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [rock, destined, 21st, centurys, new, conan, h...   \n",
       "1      [gorgeously, elaborate, continuation, lord, ri...   \n",
       "2                          [effective, tootepid, biopic]   \n",
       "3      [sometimes, like, go, movies, fun, wasabi, goo...   \n",
       "4      [emerges, something, rare, issue, movie, thats...   \n",
       "...                                                  ...   \n",
       "10657  [picture, murder, numbers, easy, bored, abcs, ...   \n",
       "10658  [hilarious, musical, comedy, though, stymied, ...   \n",
       "10659  [splatter, movies, probably, reasonably, good,...   \n",
       "10660  [dull, simpleminded, stereotypical, tale, drug...   \n",
       "10661  [featurelength, stretch, strains, shows, concept]   \n",
       "\n",
       "                                               token_ids  \n",
       "0      [533, 2417, 3118, 10163, 34, 7008, 215, 148, 1...  \n",
       "1      [3119, 1951, 7010, 4374, 2724, 4375, 948, 7011...  \n",
       "2                                     [562, 10167, 1785]  \n",
       "3               [152, 5, 107, 18, 56, 7014, 9, 212, 535]  \n",
       "4      [1205, 36, 276, 1641, 3, 41, 399, 5349, 3663, ...  \n",
       "...                                                  ...  \n",
       "10657  [84, 707, 1201, 277, 1546, 20356, 113, 20357, ...  \n",
       "10658       [370, 1231, 10, 59, 20358, 2789, 2134, 9050]  \n",
       "10659          [20359, 18, 227, 1456, 9, 11, 4260, 1042]  \n",
       "10660  [172, 4341, 3626, 87, 1792, 343, 20360, 6979, ...  \n",
       "10661                       [2154, 6673, 3450, 313, 504]  \n",
       "\n",
       "[10662 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "mr_all[\"token_ids\"] = mr_all[\"tokens\"].apply(lambda tokens: tokens_to_ids(tokens, vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rock, destined, 21st, centurys, new, conan, h...</td>\n",
       "      <td>[533, 2417, 3118, 10163, 34, 7008, 215, 148, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[gorgeously, elaborate, continuation, lord, ri...</td>\n",
       "      <td>[3119, 1951, 7010, 4374, 2724, 4375, 948, 7011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "      <td>[effective, tootepid, biopic]</td>\n",
       "      <td>[562, 10167, 1785]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[sometimes, like, go, movies, fun, wasabi, goo...</td>\n",
       "      <td>[152, 5, 107, 18, 56, 7014, 9, 212, 535]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[emerges, something, rare, issue, movie, thats...</td>\n",
       "      <td>[1205, 36, 276, 1641, 3, 41, 399, 5349, 3663, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>this picture is murder by numbers , and as eas...</td>\n",
       "      <td>0</td>\n",
       "      <td>[picture, murder, numbers, easy, bored, abcs, ...</td>\n",
       "      <td>[84, 707, 1201, 277, 1546, 20356, 113, 20357, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>hilarious musical comedy though stymied by acc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hilarious, musical, comedy, though, stymied, ...</td>\n",
       "      <td>[370, 1231, 10, 59, 20358, 2789, 2134, 9050]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>if you are into splatter movies , then you wil...</td>\n",
       "      <td>0</td>\n",
       "      <td>[splatter, movies, probably, reasonably, good,...</td>\n",
       "      <td>[20359, 18, 227, 1456, 9, 11, 4260, 1042]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>a dull , simple-minded and stereotypical tale ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dull, simpleminded, stereotypical, tale, drug...</td>\n",
       "      <td>[172, 4341, 3626, 87, 1792, 343, 20360, 6979, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>the feature-length stretch . . . strains the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[featurelength, stretch, strains, shows, concept]</td>\n",
       "      <td>[2154, 6673, 3450, 313, 504]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      the rock is destined to be the 21st century's ...      1   \n",
       "1      the gorgeously elaborate continuation of \" the...      1   \n",
       "2                         effective but too-tepid biopic      1   \n",
       "3      if you sometimes like to go to the movies to h...      1   \n",
       "4      emerges as something rare , an issue movie tha...      1   \n",
       "...                                                  ...    ...   \n",
       "10657  this picture is murder by numbers , and as eas...      0   \n",
       "10658  hilarious musical comedy though stymied by acc...      0   \n",
       "10659  if you are into splatter movies , then you wil...      0   \n",
       "10660  a dull , simple-minded and stereotypical tale ...      0   \n",
       "10661  the feature-length stretch . . . strains the s...      0   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [rock, destined, 21st, centurys, new, conan, h...   \n",
       "1      [gorgeously, elaborate, continuation, lord, ri...   \n",
       "2                          [effective, tootepid, biopic]   \n",
       "3      [sometimes, like, go, movies, fun, wasabi, goo...   \n",
       "4      [emerges, something, rare, issue, movie, thats...   \n",
       "...                                                  ...   \n",
       "10657  [picture, murder, numbers, easy, bored, abcs, ...   \n",
       "10658  [hilarious, musical, comedy, though, stymied, ...   \n",
       "10659  [splatter, movies, probably, reasonably, good,...   \n",
       "10660  [dull, simpleminded, stereotypical, tale, drug...   \n",
       "10661  [featurelength, stretch, strains, shows, concept]   \n",
       "\n",
       "                                               token_ids  \n",
       "0      [533, 2417, 3118, 10163, 34, 7008, 215, 148, 1...  \n",
       "1      [3119, 1951, 7010, 4374, 2724, 4375, 948, 7011...  \n",
       "2                                     [562, 10167, 1785]  \n",
       "3               [152, 5, 107, 18, 56, 7014, 9, 212, 535]  \n",
       "4      [1205, 36, 276, 1641, 3, 41, 399, 5349, 3663, ...  \n",
       "...                                                  ...  \n",
       "10657  [84, 707, 1201, 277, 1546, 20356, 113, 20357, ...  \n",
       "10658       [370, 1231, 10, 59, 20358, 2789, 2134, 9050]  \n",
       "10659          [20359, 18, 227, 1456, 9, 11, 4260, 1042]  \n",
       "10660  [172, 4341, 3626, 87, 1792, 343, 20360, 6979, ...  \n",
       "10661                       [2154, 6673, 3450, 313, 504]  \n",
       "\n",
       "[10662 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Convert to list of tensors\n",
    "sequence_tensors = [torch.tensor(seq) for seq in mr_all[\"token_ids\"]]\n",
    "\n",
    "# Pad\n",
    "padded_seqs = pad_sequence(sequence_tensors, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "label_tensor = torch.tensor(mr_all[\"label\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  533,  2417,  3118,  ...,     0,     0,     0],\n",
       "        [ 3119,  1951,  7010,  ...,     0,     0,     0],\n",
       "        [  562, 10167,  1785,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [20359,    18,   227,  ...,     0,     0,     0],\n",
       "        [  172,  4341,  3626,  ...,     0,     0,     0],\n",
       "        [ 2154,  6673,  3450,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, padded_seqs, labels):\n",
    "        self.seqs = padded_seqs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataset(padded_seqs, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume `padded_seqs` and `label_tensor` are torch.Tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_seqs, label_tensor, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset  = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word:idx+1 for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 400\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(20361, 400)\n",
       "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded = torch.round(torch.sigmoid(preds))  # sigmoid to get 0–1\n",
    "    correct = (rounded == y).float()\n",
    "    return correct.sum() / len(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x).squeeze(1)  # [batch]\n",
    "        loss = criterion(preds, y)\n",
    "        acc = binary_accuracy(preds, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device).float()\n",
    "            preds = model(x).squeeze(1)\n",
    "            loss = criterion(preds, y)\n",
    "            acc = binary_accuracy(preds, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62865/1949256217.py:21: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, -1]' is invalid for input of size 663",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([each\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m h])   \n\u001b[1;32m     17\u001b[0m net\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m output, h \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39msqueeze(), labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/baka/experiments/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/baka/experiments/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 25\u001b[0m, in \u001b[0;36mSentimentLSTM.forward\u001b[0;34m(self, input_words)\u001b[0m\n\u001b[1;32m     23\u001b[0m fc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_out)                      \u001b[39m# (batch_size*seq_length, n_output)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m sigmoid_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(fc_out)              \u001b[39m# (batch_size*seq_length, n_output)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m sigmoid_out \u001b[39m=\u001b[39m sigmoid_out\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# (batch_size, seq_length*n_output)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# extract the output of ONLY the LAST output of the LAST element of the sequence\u001b[39;00m\n\u001b[1;32m     28\u001b[0m sigmoid_last \u001b[39m=\u001b[39m sigmoid_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]               \u001b[39m# (batch_size, 1)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, -1]' is invalid for input of size 663"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 4  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion)\n\u001b[1;32m      5\u001b[0m     \u001b[39m#val_loss, val_acc = evaluate(model, val_loader, criterion)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[65], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m preds \u001b[39m=\u001b[39m model(x)\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# [batch]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(preds, y)\n\u001b[1;32m     12\u001b[0m acc \u001b[39m=\u001b[39m binary_accuracy(preds, y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    #val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
